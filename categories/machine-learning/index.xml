<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine-learning on Blog Needs a Name</title><link>https://ahgamut.github.io/categories/machine-learning/</link><description>Recent content in machine-learning on Blog Needs a Name</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 24 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://ahgamut.github.io/categories/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Netpicking Part 3: Prediction Sets</title><link>https://ahgamut.github.io/2021/11/24/netpicking-3/</link><pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate><guid>https://ahgamut.github.io/2021/11/24/netpicking-3/</guid><description>It&amp;rsquo;s been a while since I looked at mnistk. Recently, I came across this interesting paper which describes something called Regularized Adaptive Prediction Sets (RAPS), a technique for uncertainty estimation. The RAPS algorithm wraps a model&amp;rsquo;s prediction scores to output a set that contains the true value with high probability for a given confidence level $\alpha$. Basically, instead of getting a single class output (&amp;ldquo;this image is a squirrel&amp;rdquo;), we can now get a set.</description></item><item><title>Netpicking Part 2: Generating the networks</title><link>https://ahgamut.github.io/2020/11/24/netpicking-2/</link><pubDate>Tue, 24 Nov 2020 00:00:00 +0000</pubDate><guid>https://ahgamut.github.io/2020/11/24/netpicking-2/</guid><description>In Netpicking Part 1, I described a dilemma in picking a neural network for MNIST. I went through summary stats for 1001 different generated networks. This post explains how I generated these networks.
Representing a Neural Network The MNIST problem requires finding a function
$$ \mathit{f} : \R^{784} \rightarrow {0,1,2,3,4,5,6,7,8,9} $$
such that $ \mathit{f} $ performs well on a target dataset. I can solve this as a multi-class classification problem using neural networks, and constrain the space of functions $ \mathit{f} $:</description></item><item><title>Netpicking Part 1: Hello MNIST</title><link>https://ahgamut.github.io/2020/11/20/netpicking-1/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://ahgamut.github.io/2020/11/20/netpicking-1/</guid><description>The Hello World! introduction to neural network libraries has the user write a small network for the MNIST dataset, train it, test it, get 90% accuracy or more, and thereby get a feel for how the library works. When I started using PyTorch, I followed such a tutorial on their website. But I wondered why a network with Conv2d and ReLU was picked in the tutorial. Why not a different convolution or a Linear layer with a Sigmoid activation?</description></item></channel></rss>