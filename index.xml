<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog Needs a Name</title><link>https://ahgamut.github.io/</link><description>Recent content on Blog Needs a Name</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 13 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://ahgamut.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Patching GCC to build Actually Portable Executables</title><link>https://ahgamut.github.io/2023/07/13/patching-gcc-cosmo/</link><pubDate>Thu, 13 Jul 2023 00:00:00 +0000</pubDate><guid>https://ahgamut.github.io/2023/07/13/patching-gcc-cosmo/</guid><description>2023-07-13: I wrote a ~2000-line gcc patch to simplify building Actually Portable Executables with Cosmopolitan Libc. Now you can build popular software such as bash, curl, git, ninja, and even gcc itself, with Cosmopolitan Libc via the ./configure or cmake build system, without having to change source code, and the built executables should run on Linux, FreeBSD, MacOS, OpenBSD, NetBSD, and Windows too1. You can download the binaries built using Github Actions here: https://github.</description></item><item><title>Debugging C With Cosmopolitan Libc</title><link>https://ahgamut.github.io/2022/10/23/debugging-c-with-cosmo/</link><pubDate>Sun, 23 Oct 2022 00:00:00 +0000</pubDate><guid>https://ahgamut.github.io/2022/10/23/debugging-c-with-cosmo/</guid><description>Cosmopolitan Libc provides a suite of debugging features that enhance the C development experience: function call tracing, gdb integration, an ASAN/UBSAN runtime, and more! A lot of fast and critical code is written in C &amp;ndash; If you&amp;rsquo;re using software written in C, interfacing with C libraries, fixing bugs in C code, or even rewriting C software in some other language, it helps to understand what your C code is doing.</description></item><item><title>Actually Portable Executables with Rust and Cosmopolitan Libc</title><link>https://ahgamut.github.io/2022/07/27/ape-rust-example/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://ahgamut.github.io/2022/07/27/ape-rust-example/</guid><description>aka &amp;ldquo;Rust is Actually Portable&amp;rdquo;, after Lua and Python
I just built a Rust executable that runs on six operating systems (Linux, Windows, MacOS, FreeBSD, NetBSD, OpenBSD). If you&amp;rsquo;d like to build it yourself, clone this repo and follow the README &amp;ndash; you&amp;rsquo;ll need a recent gcc, ld.bfd, objcopy, bash, and the latest (nightly) versions of cargo, rustc, and friends.
I&amp;rsquo;ve been recently getting into Rust, and it seems pretty cool!</description></item><item><title>Netpicking Part 3: Prediction Sets</title><link>https://ahgamut.github.io/2021/11/24/netpicking-3/</link><pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate><guid>https://ahgamut.github.io/2021/11/24/netpicking-3/</guid><description>It&amp;rsquo;s been a while since I looked at mnistk. Recently, I came across this interesting paper which describes something called Regularized Adaptive Prediction Sets (RAPS), a technique for uncertainty estimation. The RAPS algorithm wraps a model&amp;rsquo;s prediction scores to output a set that contains the true value with high probability for a given confidence level $\alpha$. Basically, instead of getting a single class output (&amp;ldquo;this image is a squirrel&amp;rdquo;), we can now get a set.</description></item><item><title>Python is Actually Portable</title><link>https://ahgamut.github.io/2021/07/13/ape-python/</link><pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate><guid>https://ahgamut.github.io/2021/07/13/ape-python/</guid><description>Update (2022-07-27) : This post describes a proof-of-concept Python executable (2.7.18 and 3.6.14) built on Cosmopolitan Libc, which allows it to run on six different operating systems (Linux, Mac, Windows, NetBSD, FreeBSD, OpenBSD). It&amp;rsquo;s been more than a year since I put this together, and now Python3.6 and its test suite are part of the Cosmopolitan Libc monorepo. There&amp;rsquo;s been a LOT of work done to improve python.com over vanilla Python3.</description></item><item><title>Actually Portable Executables</title><link>https://ahgamut.github.io/2021/02/27/ape-cosmo/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>https://ahgamut.github.io/2021/02/27/ape-cosmo/</guid><description>I came across Cosmopolitan on Hacker News, and I was initially confused, due to a few memories of cross-compilation nightmares: while it should be possible to compile for the same architecture regardless of operating system, wouldn&amp;rsquo;t the OS get confused by the leading bytes of the executable? I read the article explaining how it works, but most of it went over my head.
The example on the Github README used the following script for compilation:</description></item><item><title>Netpicking Part 2: Generating the networks</title><link>https://ahgamut.github.io/2020/11/24/netpicking-2/</link><pubDate>Tue, 24 Nov 2020 00:00:00 +0000</pubDate><guid>https://ahgamut.github.io/2020/11/24/netpicking-2/</guid><description>In Netpicking Part 1, I described a dilemma in picking a neural network for MNIST. I went through summary stats for 1001 different generated networks. This post explains how I generated these networks.
Representing a Neural Network The MNIST problem requires finding a function
$$ \mathit{f} : \R^{784} \rightarrow {0,1,2,3,4,5,6,7,8,9} $$
such that $ \mathit{f} $ performs well on a target dataset. I can solve this as a multi-class classification problem using neural networks, and constrain the space of functions $ \mathit{f} $:</description></item><item><title>Netpicking Part 1: Hello MNIST</title><link>https://ahgamut.github.io/2020/11/20/netpicking-1/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://ahgamut.github.io/2020/11/20/netpicking-1/</guid><description>The Hello World! introduction to neural network libraries has the user write a small network for the MNIST dataset, train it, test it, get 90% accuracy or more, and thereby get a feel for how the library works. When I started using PyTorch, I followed such a tutorial on their website. But I wondered why a network with Conv2d and ReLU was picked in the tutorial. Why not a different convolution or a Linear layer with a Sigmoid activation?</description></item><item><title>About Me</title><link>https://ahgamut.github.io/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ahgamut.github.io/about/</guid><description>My name is Gautham. I like programming with numbers. This blog is for me to practice writing. I am currently a graduate student in the Statistics program at Iowa State University.
My username on Github is ahgamut. If you&amp;rsquo;re at this page, either you know me already or you got here via my blog posts. You can contact me at github-username &amp;lt;at&amp;gt; gmail &amp;lt;dot&amp;gt; com. I am looking for internships in the winter/summer.</description></item><item><title>Disclaimer</title><link>https://ahgamut.github.io/disclaimer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ahgamut.github.io/disclaimer/</guid><description>I don&amp;rsquo;t know.</description></item></channel></rss>